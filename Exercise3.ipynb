{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import numpy as np",
      "metadata": {
        "trusted": true
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd",
      "metadata": {
        "trusted": true
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "%pip install seaborn",
      "metadata": {
        "trusted": true
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import numpy as np",
      "metadata": {
        "trusted": true
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt",
      "metadata": {
        "trusted": true
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import seaborn as sns",
      "metadata": {
        "trusted": true
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Exercise 1: \n*Give three real-world examples of different types of cognitive bias.*",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<div style=\"text-align: justify\"> Cognitive biases are unconscious errors in thinking that arise from problems related to memory, attention, and other mental mistakes. </br>\nConfirmation bias - We tend to read online news articles that support our belief and fail to seek out sources that challenge them. </br>\nHindsight bias - When sports fans know the outcome of a game, they often question certain decisions coaches make that they otherwise would not have questioned or second guessed. </br>\nSelf-serving bias - If a driver cuts in front of you as the light turns green, the fundamental attribution error might cause you to think that they are a bad person and not consider the possibility that they were late for work.</div> </br>",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Exercise 2: \n*Show that the difference between the standard deviation calculations is greatest for small sample sizes.*",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<div style=\"text-align: justify\"> This question looks at proving the Central Limit Theory. This theory simplified is that as n gets larger and larger, the sample means follow a normal distribution. The larger n gets, the standard deviation of the sampling distribution reduces. The following plot is for a normal distribution of individual observations, and we would expect the sampling distribution to converge on the normal quickly. The results show this and show that even at a very small sample size the distribution is close to the normal distribution.</div> ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# number of sample\nnum = [1, 10, 50, 100] \n# list of sample means\nmeans = [] \n \n# Generating 1, 10, 30, 100 random numbers from -40 to 40\n# taking their mean and appending it to list means.\nfor j in num:\n    # Generating seed so that we can get same result\n    # every time the loop is run...\n    numpy.random.seed(1)\n    x = [numpy.mean(\n        numpy.random.randint(\n            -40, 40, j)) for _i in range(1000)]\n    means.append(x)\nk = 0\n \n# plotting all the means in one figure\nfig, ax = plt.subplots(2, 2, figsize =(8, 8))\nfor i in range(0, 2):\n    for j in range(0, 2):\n        # Histogram for each x stored in means\n        ax[i, j].hist(means[k], 10, density = True)\n        ax[i, j].set_title(label = num[k])\n        k = k + 1\n plt.show()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## References:\nhttps://www.simplypsychology.org/cognitive-bias.html  </br>\n\nhttps://stats.libretexts.org/Bookshelves/Applied_Statistics/Introductory_Business_Statistics_(OpenStax)/07%3A_The_Central_Limit_Theorem/7.02%3A_Using_the_Central_Limit_Theorem </br>\n\nhttps://www.geeksforgeeks.org/python-central-limit-theorem/  </br>\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}
